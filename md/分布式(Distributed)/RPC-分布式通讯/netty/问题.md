属性的所属，共享还是独占

Netty是如何处理新连接接入事件的？

Netty服务端接收的新连接是如何绑定到worker线程池的？

Netty在接收完新连接后，默认为何要为其注册读事件，其处理I/O事件的优先级是什么？

####  Netty如何封装Socket客户端Channel，Netty的Channel都有哪些类型？

#### Netty服务端接收的新连接是如何绑定到worker线程池的？

NioMessageUnsafe#read 开始

这个服务端的handler配置一般很少用到（即.handler() API），常用的主要是给客户端配置handler，即.childHandler()

head->用户定义的入站handler->ServerBootstrapAcceptor->tail

将注册逻辑写在了一个死循环里，学会这种用法，目的是为了保证一个事情必须完成，即使出现某些异常。

> 注意：服务启动走 server相关和新连接接入走 client相关

理解ServerBootstrapAcceptor:

1.延迟添加childHandler——将自定义ChannelHandler添加到新连接的pipeline，必须等当前Channel注册I/O多路复用器完毕后，才会添加

2.设置options和attrs——设置childOptions和childAttrs

3.选择NioEventLoop并注册到Selector，核心是调用worker线程池的Chooser的next()方法选择一个NioEventLoop，通过其doRegister()方法，将新连接注册到worker线程绑定的Selector上。这里的新连接和Selector是多对一的关系。



pipeline.fireChannelActive()，只有当`该Channel`是第一次注册I/O多路复用器时才会触发，后续取消再注册，再活跃。





Mina和Netty：Accept事件由一个专用的I/O线程响应——reactor线程，

Read>Write







##### NIO的I/O事件都有哪些，它们的本质是什么，处理它们有哪些坑需要注意？

一般的，服务端Channel主要就是注册OP_ACCEPT事件，客户端Channel主要注册OP_READ事件，OP_CONNECT事件，和OP_WRITE事件等。



在往底层看，TCP/IP协议栈只有两种I/O事件，就是读和写。因此JDK中的OP_ACCEPT和OP_CONNECT事件都是在读、写事件的基础上Java自己引入的，是Java自己的东西，本质上：

1、OP_ACCEPT等价于OP_READ，都属于操作系统底层的I/O读事件

2、OP_CONNECT等价于OP_WRITE，都属于操作系统底层的I/O写事件



**哪些常见的坑？**



1、既然知道了OP_ACCEPT等价于OP_READ，OP_CONNECT等价于OP_WRITE，所以不能同时两两的注册它们，这些事件映射在不同的操作系统上，表现可能不一致，最好是一个一个的注册>处理>取消注册

2、OP_WRITE事件的坑和正确用法，h以及网络较差的情况下，Netty是如何处理写事件的？

3、给I/O多路复用器注册Channel时，注册的过程本身不是线程安全的，需要业务层自己处理，为此Netty使用了reactor模型，单线程局部串行无锁化的执行所有的I/O操作，包括注册I/O事件等，让它们排队（进入MPSCQ）等待被安全执行，避免了加锁的性能损耗

4、注册I/O事件的API，本身在不同的操作系统下，表现不一致



Netty的处理方案是：屏蔽操作系统的差异，仍然是使用单线程的局部串行无锁化模型，避免key.interestOps这个API内部可能的锁竞争。

OP_CONNECT，它的使用也需要特别注意，尤其在常用的非阻塞模式下，NIO的connect方法可能会返回false，即需要额外的判断，并特殊处理



1、JDK NIO的OP_WRITE事件处理不对，很容易发生“无限”循环的问题！

2、在网络不给力的情况下，往处于非阻塞模式下的连接上调用写方法容易导致CPU被浪费，服务器性能会陡然下降！



JDK NIO的OP_WRITE事件何时会被触发，前提是必须在注册了Channel的I/O多路复用器上注册了OP_WRITE事件，之后该连接上：

1、Socket的缓冲区有空闲位置

2、对端关闭了该连接

3、该连接自己内部出现了错误



1、假设一个服务器在成功接收到一个客户端新连接后，就给它注册了OP_WRITE事件，此时可能会发生什么问题呢？

答案是可能导致“死”循环发生，最终结果就是CPU利用率达到100%，服务被拖垮！因为一个Channel上写事件的就绪条件为TCP写缓冲区有空闲位置，根据常识我们也知道TCP写缓冲区在大多场景下，都是有空闲位置的，所以直接给新连接注册写事件，那么这个写事件在大多数时间下会一直被触发，处理这个过程的I/O线程就会被长时间拖累，直到占用整个CPU资源。



合理的做法是：

1、JDK NIO的OP_WRITE事件只有在有数据需要写出的场景，才注册到对应Channel上

2、大前提是这个Channel必须活跃

3、在触发OP_WRITE事件后，业务层应该及时处理这个事件，一般交给I/O线程处理，并且处理完立即取消OP_WRITE事件的注册，然后做判断：

- 当前需要写出的数据，一次发送不完，那么需要重新注册OP_WRITE事件，即循环的注册-写-取消-判断-。。。
- 当前需要写出的数据，已经发送完，那么就无需再次注册写事件

**注册、触发写事件和什么时候写出没有直接关系**?

给组件注册XXX事件，仅仅是事件驱动模型的一种编程思想，不代表xxx事件一定会发生。比如写事件，写事件被触发，不代表有数据在此时此刻已经写出，它仅仅是告诉I/O多路复用器，此时某些连接上的缓冲区有空闲位置可放（写）数据。即这个注册写事件的过程是I/O多路复用器需要的，当某个Channel上注册了相关的I/O事件，就可以通过Selector的select(xxx)方法轮询出发生该事件的那些Channel，之后业务上做相应判断和处理即可。



理解为什么需要注册事件?

以写事件为例，给某个Channel注册写事件的目的是为了查看当前Channel的缓冲区是否可以写数据，这个触发时机就是底层缓冲区有空闲位置。



Channel的write方法并不可靠，即不一定真的会写出数据，比如在非阻塞模式下，该方法不会阻塞。假设网络环境很差，业务层一直在发数据，TCP的发送缓冲区很快会满，这一般是由滑动窗口等流量控制机制决定的，缓冲区满就会拒绝新数据写入。此时调用Channel的write方法就会立即返回0

JDK的注释:：尝试向该Channel中写入最多r个字节，r是调用此方法时缓冲区中剩余的字节数，即src.remaining()返回值，假设写入了长度为n的字节序列，其中0<=n<=r。从缓冲区的索引p处开始传输该字节，其中p是调用此方法时该缓冲区的位置；最后写入的字节索引是p+n-1。返回时该缓冲区的位置将等于p+n；限制不会更改。除非另行指定，否则仅在写入所有请求的r个字节后write操作才会返回。有些类型的Channel（取决于它们的状态）可能仅写入某些字节或者可能根本不写入。例如处于非阻塞模式的SocketChannel只能写入该套接字输出缓冲区中的字节。可在任意时间调用此方法。但是如果另一个线程已经在此Channel上发起了一个写操作，则在该操作完成前此方法的调用被阻塞。



需要正确理解这个过程。简单说，在发送缓冲区空间不够时，write方法返回的字节数可能只是需要写出数据的一部分，比如写缓冲区只剩100字节空间，写入200字节，write返回100，如果缓冲区满，那么write返回0。在正常情况下不太可能发生上述问题，就怕网络不好的时候，此时数据包重传率非常高，发送数据的I/O线程会一直被拖累在这里，



这时候注册OP_WRITE事件就有用了！NIO编程中比较常用的套路如下：

1、在socketChannel.write返回0时，给此Channel注册OP_WRITE事件，然后马上退出循环，让I/O线程去做别的事情

2、当网络恢复正常后，该Channel的底层写缓冲区会变为非满，此时触发Channel上的写事件，通知Selector，业务上就可以让I/O线程来处理写数据的操作，这样就能节约大量CPU资源，服务器也能适应恶劣的网络环境，非常健壮了。



##### JDK Selector的select()方法返回值，理解NIO编程的一些坑点

https://mp.weixin.qq.com/s/C2PKHks3LXDGlVDAK5msng

1、Selector的select到底返回的是什么？

本质是selectionKey集合的元素个数

2、select这个阻塞API，什么时候会解除阻塞而返回？

三个条件，和一个坑点

3、select这个阻塞API，什么时候返回0？

一个坑点

4、如果在检测到就绪的Channel后，获取key，业务不处理，那么有什么问题？

可能导致死循环bug

5、如果在检测到就绪的Channel后，获取key，业务处理后，没有删除就绪集合里的这个key，那么又会怎么样呢？

可能导致数据混乱，或者select不阻塞，导致触发CPU空转



1、select()返回的是自从上次select后，刚刚被触发的I/O事件，所关联的Channel的key的集合的大小。有些绕，结合demo理解。

2、如果在调用select之前，key的集合已经有元素了（一句话就是上一次调用select后，key集合没有删干净），那么此时select检测到就绪的Channel后，会返回0。隐患就是如果你判断它返回值<=0就继续检测，那么可能导致CPU空转，使用率100%。

3、更本质的说，Selector在操作系统层面使用了两个事件表，还记得之前分析Selector的源码时提到的poll数组么，这是一个注册表，当你调用channel.register时，会有一个新的元素塞入到poll数组，这个元素在JVM就是对应了selectionkey，该表不会自己清理，必须用户手动清理，对应到JDK层面就是使用key的cancel方法取消它。还有一个就绪事件表，保存已经就绪的key，两个表的转移，就是通过Selector的select调用，即Selector检测poll数组后，将里面有I/O事件就绪的Channel的fd复制到就绪事件表，对应到JDK层面就是publicSelectedKeys，而这是一个HashSet集合，它在JDK里被包装为了一个只能remove的对象，如果每轮检测后，你处理但是不remove掉对应的key，那么下次检测时，还会被Selector感知到这个key，对应到代码就是selector.selectedkeys()返回的集合里仍然包括了上次遗留的key。所以，NIO编程的一个套路是必须在处理完一个key后，remove它，如果确实有连续处理某个Channel的剩余数据的需求，那么可以remove后，等下次检测到被触发，再拿出来处理，因为JDK选择了LT工作模式。你不用担心下次检测不出来。一句话，一轮检测后，把key全部remove就完事儿了。



wakenUp被声明为了JUC的原子类，用来标识是否应该唤醒阻塞在NIO select API上的NIO线程。对于这个变量的，它的存在目的是为了规避重复调用wakeup方法。



NIO线程不仅要轮询Channel的I/O事件，并处理I/O事件，还要串行的执行MPSCQ里的task，那么多事情要做，Netty是如何判断各个事情的执行时机的呢？



##### 恶劣的网络环境下，Netty是如何处理写事件的

位与运算判断并清理注册的I/O事件。





channelRead以及channelReadComplete方法，下面做个总结：

1、尽量推荐使用channelRead方法，慎用channelReadComplete

2、因为它可能在配置了解包器的程序中，由于使用不当而出现bug，比如常用的ByteToMessageDecoder解码器，以及它的子类实现们，这些解码器在一次处理读事件时，可能一次调用JDK的SocketChannel的read后，得到的数据并不是一个完整的包，Netty会用一个循环来持续积累剩余的数据字节，直到这些字节能拼凑一个包后，才继续传播回调channelRead事件，而期间channelReadComplete事件确可能被调用多次，即一句话总结：channelRead的回调时机是可以跟着业务规则来的，channelReadComplete则不可以。

3、同样的场景，还有HTTP协议的通信，在基于Netty开发HTTP服务器或者客户端时，通常都会用到Netty提供的HTTP协议的解码器——HttpObjectAggregator，目的保证业务的入站ChannelHandler接收的是完整的 HTTP报文，如下使用：

```java
/** 解析 HTTP 请求 */
pipeline.addLast(new HttpServerCodec());
//主要是将同一个http请求或响应的多个消息对象变成一个 fullHttpRequest完整的消息对象
pipeline.addLast(new HttpObjectAggregator(64 * 1024));
//主要用于处理大数据流,比如一个1G大小的文件如果你直接传输肯定会撑暴jvm内存的 ,使用该 handler我们就不用考虑这个问题了
pipeline.addLast(new ChunkedWriteHandler());
// 自定义处理
pipeline.addLast(new HttpHandler());

/** 解析 webSocket 请求 */
pipeline.addLast(new WebSocketServerProtocolHandler("/im"));
// 自定义处理
pipeline.addLast(new WebSocketHandler());
```



客户端接收服务器的HTTP响应，一个完整的HTTP响应报文可能是调用了2次JDK的SocketChannel的read方法后，才接收完毕的，此时业务的入站handler的channelReadComplete事件就会被回调2次，而channelRead事件只会被回调一次，此时业务逻辑若依赖了channelReadComplete，就肯定会出现异常。



尤其是发送大数据流，比如采用chunked方式发送HTTP请求报文，就更有可能引发上述问题了。





##### Netty的handler可以删除么，在什么时候应该删除，又是如何删除的？

pipeline的remove

getContextOrDie（handler）,不能删除头尾节点,从头开始遍历

回调删除handler的事件

从remove0出来，会进入重载的remove

```java
// DefaultChannelPipeline
@Override
public final ChannelPipeline remove(ChannelHandler handler) {
    remove(getContextOrDie(handler));
    return this;
}

private AbstractChannelHandlerContext getContextOrDie(ChannelHandler handler) {
        AbstractChannelHandlerContext ctx = (AbstractChannelHandlerContext) context(handler);
        if (ctx == null) {
            throw new NoSuchElementException(handler.getClass().getName());
        } else {
            return ctx;
        }
    }

@Override
public final ChannelHandlerContext context(ChannelHandler handler) {
    if (handler == null) {
        throw new NullPointerException("handler");
    }

    AbstractChannelHandlerContext ctx = head.next;
    for (;;) {

        if (ctx == null) {
            return null;
        }

        if (ctx.handler() == handler) {
            return ctx;
        }

        ctx = ctx.next;
    }
}


private AbstractChannelHandlerContext remove(final AbstractChannelHandlerContext ctx) {
        assert ctx != head && ctx != tail;

        synchronized (this) {
            atomicRemoveFromHandlerList(ctx);

            // If the registered is false it means that the channel was not registered on an eventloop yet.
            // In this case we remove the context from the pipeline and add a task that will call
            // ChannelHandler.handlerRemoved(...) once the channel is registered.
            if (!registered) {
                // 
                callHandlerCallbackLater(ctx, false);
                return ctx;
            }

            EventExecutor executor = ctx.executor();
            if (!executor.inEventLoop()) {
                executor.execute(new Runnable() {
                    @Override
                    public void run() {
                        // 
                        callHandlerRemoved0(ctx);
                    }
                });
                return ctx;
            }
        }
        callHandlerRemoved0(ctx);
        return ctx;
    }
```



#####  如何理解Netty的ChannelInitializer的设计，它为pipeline添加handler是在何时被调用的？

认识特殊的抽象类ChannelInitializer

 initChannel方法是何时被调用的？

 实际上会在handlerAdded事件回调时，就会调用到用户覆写的initChannel方法，注意这里的initChannel是一个私有的重载方法，会在这个重载方法里回调用户覆写的那个抽象的initChannel方法。ChannelInitializer这个入站handler会被Netty删除，因为它已经完成了它的使命，这样可以节省资源，这是Netty的优化点

###### 学习Netty对延迟调用的设计 pending task

学习Netty区分底层系统的设计思路

认识ConcurrentHashMapV8，它和ConcurrentHashMap有什么区别？



1、它本质是一个入站handler，继承了ChannelInboundHandlerAdapter

2、它是一个可被多个channel共享的handler，通过注解@Sharable标识，注意这个注解其实没有实际的作用，仅仅是个标记，后续专题总结，但是一定要注意，在使用它时，务必自己保障线程安全，Netty不做任何承诺。

3、它是一个泛型类，限制必须是Netty的Channel的子类，可以方便用户为服务端Channel或者客户端Channel添加handler（本质是添加到对应的pipeline）

4、用户可以在覆写的initChannel方法里添加/删除handler，一般是addXXX，

```java
private boolean initChannel(ChannelHandlerContext ctx) throws Exception {
    // 避免重复初始化   
    if (initMap.add(ctx)) { // Guard against re-entrance.
            try {
                initChannel((C) ctx.channel());
            } catch (Throwable cause) {
                // Explicitly call exceptionCaught(...) as we removed the handler before calling initChannel(...).
                // We do so to prevent multiple calls to initChannel(...).
                exceptionCaught(ctx, cause);
            } finally {
                ChannelPipeline pipeline = ctx.pipeline();
                if (pipeline.context(this) != null) {
                    // ChannelInitializer已经完成了为pipeline配置handler的使命
                    // 这些标记位都没用了，删除以节省资源
                    pipeline.remove(this);
                }
            }
            return true;
        }
        return false;
    }
```

Netty认为只有当新的Channel已经成功注册了I/O多路复用器且已经绑定到I/O线程后，才有必要为其添加用户配置的handler，否则不添加



```java
@Override
    public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) {
        final AbstractChannelHandlerContext newCtx;
        // 因为用户可能会配置外部线程执行这块添加handler的逻辑，所以此处Netty加锁
        synchronized (this) {
            // 判断当前handler是否已经添加或者说能否被正常添加
            checkMultiplicity(handler);
            // 提前给当前handler创建一个新的pipeline的节点
            newCtx = newContext(group, filterName(name, handler), handler);

            addLast0(newCtx);

            // If the registered is false it means that the channel was not registered on an eventLoop yet.
            // In this case we add the context to the pipeline and add a task that will call
            // ChannelHandler.handlerAdded(...) once the channel is registered.
            // 若还没有为 Channel 注册I/O多路复用器
            if (!registered) {
                // 新创建的节点会被设置为挂起状态，留待后续延迟处理
                newCtx.setAddPending();             
                callHandlerCallbackLater(newCtx, true);
                return this;
            }

            EventExecutor executor = newCtx.executor();
            if (!executor.inEventLoop()) {
                callHandlerAddedInEventLoop(newCtx, executor);
                return this;
            }
        }
        callHandlerAdded0(newCtx);
        return this;
    }

private static void checkMultiplicity(ChannelHandler handler) {
    // Netty所有的handler都必须是ChannelHandlerAdapter的实例，它也是ChannelHandler接口的默认实现
    if (handler instanceof ChannelHandlerAdapter) {
            
        ChannelHandlerAdapter h = (ChannelHandlerAdapter) handler;
        /**
        该handler是否被标记位能被共享，Netty规定可以在handler类上添加@Sharable注解来标记这个handler能在多个Channel的pipeline中被共享，注意Netty不做任何承诺，该注解仅仅是个标记，换句话说可以设置一个handler被多个Channel共享，但是这样设置的前提是该handler的内部操作是线程安全的，这需要设置的人自己做保证。比如可用来收集跨Channel的统计性信息，或者抽象一些共享的逻辑以节省内存等，这都是Netty的高阶用法了，后续专题总结。Netty通过自带的isSharable()方法判断是否使用了该注解，它的逻辑也很简单，即通过反射拿到handler的类的注解然后进行规则的判断，不多说
该handler是否已经被添加过，判断逻辑也很简单，看added变量，它是一个布尔变量，用来标记该handler是否被添加过了，
        */
        if (!h.isSharable() && h.added) {
                throw new ChannelPipelineException(
                        h.getClass().getName() +
                        " is not a @Sharable handler, so can't be added or removed multiple times.");
            }
            h.added = true;
        }
    }
```

这里的 if 若判断成功，因此此时还没有为服务端Channel注册I/O多路复用器，下面进入if内部，这个新创建的节点会被设置为挂起状态，留待后续延迟处理，注释也解释了——一旦成功注册到I/O多路复用器上，才触发该handler的handlerAdd事件，执行ChannelInitializer的handlerAdd方法会回调用户覆写的initChannel方法。这个延迟实现的思路是把这段逻辑封装为了异步task，塞入到了一个链表——pendingHandlerCallbackHead



```java
private void register0(ChannelPromise promise) {
            try {
                // check if the channel is still open as it could be closed in the mean time when the register
                // call was outside of the eventLoop
                if (!promise.setUncancellable() || !ensureOpen(promise)) {
                    return;
                }
                boolean firstRegistration = neverRegistered;
                doRegister();
                neverRegistered = false;
                registered = true;

                // Ensure we call handlerAdded(...) before we actually notify the promise. This is needed as the
                // user may already fire events through the pipeline in the ChannelFutureListener.
                pipeline.invokeHandlerAddedIfNeeded();

                safeSetSuccess(promise);
                pipeline.fireChannelRegistered();
                // Only fire a channelActive if the channel has never been registered. This prevents firing
                // multiple channel actives if the channel is deregistered and re-registered.
                if (isActive()) {
                    if (firstRegistration) {
                        pipeline.fireChannelActive();
                    } else if (config().isAutoRead()) {
                        // This channel was registered before and autoRead() is set. This means we need to begin read
                        // again so that we process inbound data.
                        //
                        // See https://github.com/netty/netty/issues/4805
                        beginRead();
                    }
                }
            } catch (Throwable t) {
                // Close the channel directly to avoid FD leak.
                closeForcibly();
                closeFuture.setClosed();
                safeSetFailure(promise, t);
            }
        }

final void invokeHandlerAddedIfNeeded() {
        assert channel.eventLoop().inEventLoop();
        if (firstRegistration) {
            firstRegistration = false;
            // We are now registered to the EventLoop. It's time to call the callbacks for the ChannelHandlers,
            // that were added before the registration was done.
            callHandlerAddedForAllHandlers();
        }
    }

private void callHandlerAddedForAllHandlers() {
        final PendingHandlerCallback pendingHandlerCallbackHead;
        synchronized (this) {
            assert !registered;

            // This Channel itself was registered.
            registered = true;

            pendingHandlerCallbackHead = this.pendingHandlerCallbackHead;
            // Null out so it can be GC'ed.
            this.pendingHandlerCallbackHead = null;
        }

        // This must happen outside of the synchronized(...) block as otherwise handlerAdded(...) may be called while
        // holding the lock and so produce a deadlock if handlerAdded(...) will try to add another handler from outside
        // the EventLoop.
        PendingHandlerCallback task = pendingHandlerCallbackHead;
        while (task != null) {
            task.execute();
            task = task.next;
        }
    }
// PendingHandlerAddedTask
@Override
        void execute() {
            EventExecutor executor = ctx.executor();
            if (executor.inEventLoop()) {
                callHandlerAdded0(ctx);
            } else {
                try {
                    executor.execute(this);
                } catch (RejectedExecutionException e) {
                    if (logger.isWarnEnabled()) {
                        logger.warn(
                                "Can't invoke handlerAdded() as the EventExecutor {} rejected it, removing handler {}.",
                                executor, ctx.name(), e);
                    }
                    atomicRemoveFromHandlerList(ctx);
                    ctx.setRemoved();
                }
            }
        }
```

##### **为新客户端Channel配置handler时调用**

```java
// 内部类 ServerBootstrapAcceptor
@Override
@SuppressWarnings("unchecked")
public void channelRead(ChannelHandlerContext ctx, Object msg) {
    final Channel child = (Channel) msg;
    //
    child.pipeline().addLast(childHandler);

    setChannelOptions(child, childOptions, logger);
    setAttributes(child, childAttrs);

    try {
        childGroup.register(child).addListener(new ChannelFutureListener() {
            @Override
            public void operationComplete(ChannelFuture future) throws Exception {
                if (!future.isSuccess()) {
                    forceClose(child, future.cause());
                }
            }
        });
    } catch (Throwable t) {
        forceClose(child, t);
    }
}
```



#### 客户端 channel

Socket是一种"打开读/写关闭"模式的实现，是对底层TCP/IP协议栈和网卡的封装，并暴露了网络相关的API给用户层。具体来说，在服务器和客户端各自维护了一个文件，在TCP连接打开后，可以向自己机器上的这个文件写入内容，供对方读取（反之亦然），写入结束时关闭文件，也就是说客户端和服务器各有一个Socket文件，服务器的Socket保持监听的状态，客户端的Socket可以调用write()往Socket里写数据——发送过程，然后服务器Socket接到客户端传来的数据，会调用类似read()的API去读取，也就是说Socket就是应用层和传输层之间的一个抽象接口，





Netty或者说框架本身怎么才能知道用户现在在配置服务端还是客户端呢？一般情况下，可以有以下几个解决方案：

1、通过一些标记位进行标记

2、通过反射屏蔽差异

3、通过泛型屏蔽差异

4、前面两个方案结合



1、一个是良好的命名设计，看看Netty，很多方法，包括变量的命名，都非常的规范，比如initAndRegister等，都是见名知意，一目了然的，在写业务代码的时候也应该有这个意识，这样别人阅读你的代码，会非常节省精力，对你自然也会高看一眼。



2、学会复用代码的一种技巧，即在提供SDK，或者底层核心库时，如果深度设计了代码复用，导致一些复用的组件无法判断上层用户的配置，或者使用意图，那么可以使用开放class的配置，通过反射机制来屏蔽这种认知差异



1、学习规范的命名

2、善用重载方法，尽可能减轻上层调用者负担

3、善用Closeable接口关闭资源

4、合理的设计模式应用，比如职责链模式，pipeline模式，模板方法模式，工厂模式，单例模式，观察者模式等

5、在进入业务代码前，可以先做参数以及必选配置的校验，并封装为方法甚至专用的工具类，进行集中式校验







##### Netty的I/O模型（Channel）实例化的时候,都做了什么?



理解EventLoop：这个对象本身聚合了一个永远不会改变的JDK 的Thread对象，在其整个生命周期内，该线程会驱动该EventLoop里的一切动作，包括处理当前绑定的Channel（网络连接）上的所有I/O事件，当然后续会知道，还能处理一些异步任务，每一个EventLoop都是如此。



就是因为这个简单而强大的设计，巧妙的避免了多个I/O线程共享同一个Channel的场景出现。使得每个Channel的pipeline中的处理单元，能够被串行的执行，同时多个channel-pipeline在宏观上又是并行（发）执行的。

##### Netty如何解决的大量客户端连接上的线程同步问题?

这种设计实际上消除了对于一个网络连接中I/O事件同步的需要。且也不缺乏并行性，尤其在多核处理器里，更是明显，这就是大名鼎鼎的局部串行无锁化线程池的设计思想，业务代码里可以参考。

Netty的NioEventLoop和NioEventLoopGroup是什么关系?



##### Netty的接入线程池和服务端Channel是如何关联的？



I/O密集型计算场景：最佳线程数=1+（IO耗时/CPU耗时）

CPU密集型：最佳线程数=CPU核数*[1+（IO耗时/CPU耗时）]





延迟 吞吐量

Netty的应用场景主要是I/O密集型任务 ioRatio

apm







线程选择器，位运算









一个Socket上的输入操作（收消息），分为两个阶段：

1、等待数据从网络中到达，当所有数据分组到达网卡，这些数据会被复制到内核，此时叫做数据准备好了；

2、把这份数据从内核复制到进程的用户缓冲区，之后用户代码才能操作这份数据。





#### 高效轮询I/O事件的策略设计思想

https://mp.weixin.qq.com/s/Vj_PMdQow5yKpigJ3hdW_Q

1、NioEventLoop的NIO线程（后续简称NIO线程）不仅要轮询Channel的I/O事件，并处理I/O事件，还要串行的执行MPSCQ里的task，那么多事情要做，Netty是如何判断各个事情的执行时机的呢？

2、如果要定制Netty轮询Channel的策略，那么有什么办法实现么？

3、如果有一些非紧急的异步task要提交给Netty执行，即不希望打扰到Netty的I/O事件的轮询过程，那么应该怎么配置？



```java
@Override
    public void execute(Runnable task) {
        if (task == null) {
            throw new NullPointerException("task");
        }

        boolean inEventLoop = inEventLoop();
        addTask(task);
        if (!inEventLoop) {
            startThread();
            if (isShutdown()) {
                boolean reject = false;
                try {
                    if (removeTask(task)) {
                        reject = true;
                    }
                } catch (UnsupportedOperationException e) {
                    // The task queue does not support removal so the best thing we can do is to just move on and
                    // hope we will be able to pick-up the task before its completely terminated.
                    // In worst case we will log on termination.
                }
                if (reject) {
                    reject();
                }
            }
        }

        // 唤醒 NioEventLoop 线程的I/O多路复用器的阻塞的select方法
        // !addTaskWakesUp在默认的Netty线程池里总是为true
        // wakesUpForTask方法的实现很简单，判断当前task是否是无需被及时执行的任务
        // NonWakeupRunnable的设计，用户可以基于它实现无需被紧急执行的异步任务，即让Netty优先监听并处理Channel的I/O事件
        if (!addTaskWakesUp && wakesUpForTask(task)) { 
            /**
            run方法里的Boolean原子变量wakenUp是false，
            这里它为false说明该NIO线程在本轮检测Channel时还没有被唤醒过，
            且初始化时以及Netty每轮的检测开始之前，它都恢复为false，
            那么才让外部线程执行NIO的wakeup API去唤醒一次该NIO线程。
            又因为该变量为原子的共享变量，所以wakenUp的判断通过CAS实现，
            而且也顺便将wakenUp变量在比较后置为true，代表本NIO线程已经被唤醒过一次          
            */
            wakeup(inEventLoop);
        }
    }


@Override
    protected void run() {
        for (;;) {
            try {
                try {
                    /**
                    @Override
                    public int calculateStrategy(IntSupplier selectSupplier, boolean hasTasks) throws Exception {
                        return hasTasks ? selectSupplier.get() : SelectStrategy.SELECT;
                    }
                    */
                    switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) {
                    case SelectStrategy.CONTINUE:
                        continue;

                    case SelectStrategy.BUSY_WAIT:
                        // fall-through to SELECT since the busy-wait is not supported with NIO

                    case SelectStrategy.SELECT:
                        select(wakenUp.getAndSet(false));

                        // 'wakenUp.compareAndSet(false, true)' is always evaluated
                        // before calling 'selector.wakeup()' to reduce the wake-up
                        // overhead. (Selector.wakeup() is an expensive operation.)
                        //
                        // However, there is a race condition in this approach.
                        // The race condition is triggered when 'wakenUp' is set to
                        // true too early.
                        //
                        // 'wakenUp' is set to true too early if:
                        // 1) Selector is waken up between 'wakenUp.set(false)' and
                        //    'selector.select(...)'. (BAD)
                        // 2) Selector is waken up between 'selector.select(...)' and
                        //    'if (wakenUp.get()) { ... }'. (OK)
                        //
                        // In the first case, 'wakenUp' is set to true and the
                        // following 'selector.select(...)' will wake up immediately.
                        // Until 'wakenUp' is set to false again in the next round,
                        // 'wakenUp.compareAndSet(false, true)' will fail, and therefore
                        // any attempt to wake up the Selector will fail, too, causing
                        // the following 'selector.select(...)' call to block
                        // unnecessarily.
                        //
                        // To fix this problem, we wake up the selector again if wakenUp
                        // is true immediately after selector.select(...).
                        // It is inefficient in that it wakes up the selector for both
                        // the first case (BAD - wake-up required) and the second case
                        // (OK - no wake-up required).

                        if (wakenUp.get()) {
                            selector.wakeup();
                        }
                        // fall through
                    default:
                    }
                } catch (IOException e) {
                    // If we receive an IOException here its because the Selector is messed up. Let's rebuild
                    // the selector and retry. https://github.com/netty/netty/issues/8566
                    rebuildSelector0();
                    handleLoopException(e);
                    continue;
                }

                cancelledKeys = 0;
                needsToSelectAgain = false;
                final int ioRatio = this.ioRatio;
                if (ioRatio == 100) {
                    try {
                        processSelectedKeys();
                    } finally {
                        // Ensure we always run tasks.
                        runAllTasks();
                    }
                } else {
                    final long ioStartTime = System.nanoTime();
                    try {
                        processSelectedKeys();
                    } finally {
                        // Ensure we always run tasks.
                        final long ioTime = System.nanoTime() - ioStartTime;
                        runAllTasks(ioTime * (100 - ioRatio) / ioRatio);
                    }
                }
            } catch (Throwable t) {
                handleLoopException(t);
            }
            // Always handle shutdown even if the loop processing threw an exception.
            try {
                if (isShuttingDown()) {
                    closeAll();
                    if (confirmShutdown()) {
                        return;
                    }
                }
            } catch (Throwable t) {
                handleLoopException(t);
            }
        }
    }
```



不能让当前I/O线程闲着，让它及时执行事件轮询；否则，说明此时的MPSCQ里有异步任务，I/O线程应该及时去执行CPU任务，只不过在执行前，Netty又调用非阻塞的selectNow做了一次检测，如果此时此刻有Channel有I/O事件就绪，那么还是优先处理I/O事件，毕竟这个处理过程极快，不耽误异步任务的执行，反过来，CPU任务就可能没这么快，所以在进入正式的轮询逻辑之前，Netty优先处理I/O事件，而且在switch里调用一次selectNow，也避免了在后续重复调用select方法而耽误时间。。。

也就是说，Netty的事件循环机制，不一定每次循环都会进入正式的循环逻辑，它前面其实有一个微小的工作。



wakenUp被声明为了JUC的原子类，用来标识是否应该唤醒阻塞在NIO select API上的NIO线程。对于这个变量的设计后续细说，它的存在目的是为了规避重复调用wakeup方法。



```java
private void select(boolean oldWakenUp) throws IOException {
        Selector selector = this.selector;
        try {
            // 记录Netty本轮检测Channel的次数，本质是记录调用了多少次NIO的select API
            int selectCnt = 0;
            long currentTimeNanos = System.nanoTime();
            // 从定时任务队列中取出定时任务，如果有则计算离当前定时任务的下一次执行时间之差
            // 没有则按照固定的 1s 作为本次 select 的时间
            long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos);

            long normalizedDeadlineNanos = selectDeadLineNanos - initialNanoTime();
            if (nextWakeupTime != normalizedDeadlineNanos) {
                nextWakeupTime = normalizedDeadlineNanos;
            }

            for (;;) {
                // 将当前时间差(ns)转化成ms, 并且预备和0.5ms做比较, 
                // 即当前时间 * 10的-6次, 代码里直接除以了 1000000L, 
                // 其实转换的就是 delayNanos(currentTimeNanos)
                long timeoutMillis = (selectDeadLineNanos - currentTimeNanos + 500000L) / 1000000L;
                // 如当前时间差(delayNanos(currentTimeNanos))不足0.5ms，
                // 即timeoutMillis<=0
                if (timeoutMillis <= 0) {
                    // 第一次执行，则认为本次 select 的时间太短，只执行一次selectNow() 即可
                    if (selectCnt == 0) {
                        selector.selectNow();
                        selectCnt = 1;
                    }
                    break;
                }

                // If a task was submitted when wakenUp value was true, the task didn't get a chance to call
                // Selector#wakeup. So we need to check task queue again before executing select operation.
                // If we don't, the task might be pended until select operation was timed out.
                // It might be pended until idle timeout if IdleStateHandler existed in pipeline.
                if (hasTasks() && wakenUp.compareAndSet(false, true)) {
                    selector.selectNow();
                    selectCnt = 1;
                    break;
                }

                int selectedKeys = selector.select(timeoutMillis);
                selectCnt ++;

                if (selectedKeys != 0 || oldWakenUp || wakenUp.get() || hasTasks() || hasScheduledTasks()) {
                    // - Selected something,
                    // - waken up by user, or
                    // - the task queue has a pending task.
                    // - a scheduled task is ready for processing
                    break;
                }
                if (Thread.interrupted()) {
                    // Thread was interrupted so reset selected keys and break so we not run into a busy loop.
                    // As this is most likely a bug in the handler of the user or it's client library we will
                    // also log it.
                    //
                    // See https://github.com/netty/netty/issues/2426
                    if (logger.isDebugEnabled()) {
                        logger.debug("Selector.select() returned prematurely because " +
                                "Thread.currentThread().interrupt() was called. Use " +
                                "NioEventLoop.shutdownGracefully() to shutdown the NioEventLoop.");
                    }
                    selectCnt = 1;
                    break;
                }

                long time = System.nanoTime();
                if (time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) >= currentTimeNanos) {
                    // timeoutMillis elapsed without anything selected.
                    selectCnt = 1;
                } else if (SELECTOR_AUTO_REBUILD_THRESHOLD > 0 &&
                        selectCnt >= SELECTOR_AUTO_REBUILD_THRESHOLD) {
                    // The code exists in an extra method to ensure the method is not too big to inline as this
                    // branch is not very likely to get hit very frequently.
                    selector = selectRebuildSelector(selectCnt);
                    selectCnt = 1;
                    break;
                }

                currentTimeNanos = time;
            }

            if (selectCnt > MIN_PREMATURE_SELECTOR_RETURNS) {
                if (logger.isDebugEnabled()) {
                    logger.debug("Selector.select() returned prematurely {} times in a row for Selector {}.",
                            selectCnt - 1, selector);
                }
            }
        } catch (CancelledKeyException e) {
            if (logger.isDebugEnabled()) {
                logger.debug(CancelledKeyException.class.getSimpleName() + " raised by a Selector {} - JDK bug?",
                        selector, e);
            }
            // Harmless exception - log anyway
        }
    }


/**
 延迟时间的计算目的，是因为NioEventLoop不仅绑定了MPSCQ，还绑定了一个定时任务队列，该定时任务队列里的task会按照任务的deadline排序，delayNanos(long time)方法的作用是通过当前开始执行Netty的select的时间，来推算距离定时任务队列里第一个task的deadline的剩余时间，显然目的是不能在轮询Channel时，耽误了定时任务的执行。
 */
protected long delayNanos(long currentTimeNanos) {
        ScheduledFutureTask<?> scheduledTask = peekScheduledTask();
        if (scheduledTask == null) {
            return SCHEDULE_PURGE_INTERVAL;
        }
        // return Math.max(0, deadlineNanos() - (currentTimeNanos - START_TIME));
        return scheduledTask.delayNanos(currentTimeNanos);
    }
```



select()方法执行逻辑总结：

1、计算本轮检测的deadline，目的是为了保证定时任务被及时执行，也保证新的异步任务被及时响应，它们之间穿插执行，而deadline的计算是根据NioEventLoop当时是否有定时任务判断的

2、阻塞式select的调用处理技巧：未到截止时间或者MPSCQ为空则进行一次阻塞式select，期间穿插了唤醒操作的正确处理方式，避免多次重复唤醒，避免唤醒一次后，select结束后新的任务加入，会被耽误执行

3、其实后面还有一段代码，是为了规避解决epoll空轮询bug



官方阐述了产生这个bug的条件，有两个:

1、已经建立连接的Channel上，注册的I/O事件为0

2、对端非法关闭TCP连接，即发了一个RST包

selector被唤醒select不在阻塞







##### AccessController.doPrivileged

提权API包裹业务代码，它能使一段被包裹的代码获得系统上的最大权限，甚至比调用他的应用程序还多

doPrivileged可以让程序突破当前域权限限制，临时扩大访问权限。



沙箱机制就是将Java代码限定在虚拟机(JVM)特定的运行范围中，并且严格限制代码对本地系统的资源访问，通过这样的措施来保证对代码的隔离，防止对本地系统造成破坏。沙箱默认不启动，如果想使用，那么需要手动打开。



Java程序分为本地代码和远程代码，本地代码默认可信，远程代码被看作不受信的。我们需要隔离远程代码，防止它对本地系统造成破坏。假如对同一个资源，本地代码有访问权限，远程代码没有权限，JVM就一个，沙箱开关就一个，那怎么才能做到呢？因此Java安全模型引入了域，JVM会通过ClassLoader把代码加载到不同的系统域和应用域，系统域负责与关键资源进行交互，各个应用域通过系统域的部分代理来对各种需要的资源进行访问。JVM中不同的域对应不一样的权限，存在域中的类文件就有了当前域的全部权限：







强行优化底层依赖库 ：通过反射修改属性，将一些性能不够的集合类型之类的换成其他性能高德



Arrays.fill

##### 关闭Channel为何会引起CLOSE_WAIT，Netty是怎么做的？

关闭Channel的流程里，内部最终会调用key.cancel()，如下，该方法的作用是取消已经注册在某个Selector的Channel：

JDK对SocketChannel的关闭/取消过程，是一个延迟策略，直到下一次调用NIO的select时，该Channel才真正被取消。

processDeregisterQueue

如果永远都没下次的select，那么该Channel的Socket就一直停留在CLOSE_WAIT态

如果channel.close()方法导致调用了key.cancel()，该Channel的操作系统的fd真正的关闭时机也是在key被真正取消的时候，这里有一个时间的延误，当然如果永远都没下次的select，那么该Channel的Socket就一直停留在CLOSE_WAIT态，回忆TCP协议的状态。



对应到程序里，如果是服务器，那么还好，毕竟需要频繁的调用select，如果是客户端就有问题了，客户端通常只是发起一个SocketChannel，连接到对端，一旦客户端主动关闭Channel后，往往也要关闭对应的Selector，若这样的操作反复发生，且Selector都已经关闭了，那么会导致大量的CLOSE_WAIT态的Channel停留在内存，性能损耗会飙升！



而Netty就考虑到了这一点，将取消注册计数，在适当的时候，Netty是256次后，主动调用一次selectNow，以方便关闭需要关闭的Channel，以及及时取消无效的key。否则大量的key已经标识为失效，但是还存在于系统中。这样就能一方面保证现存的SelectionKey是即时有效的，一方面保证需要关闭的Channel被及时的真正关闭。



为什么要在处理256次I/O事件后（本质是selectionKey），就结束处理过程。

#### 学习普通异步任务和定时任务汇聚处理的策略思想

AbstractScheduledEventExecutor

scheduledTaskQueue

scheduled

ScheduledFutureTask



runAllTasks

fetchFromScheduledTaskQueue方法聚合了定时任务

periodNanos



evenloop#execute  addTask   offerTask  taskQueue#offer 添加普通异步任务

channel register

注册逻辑是在服务端启动时触发，会被封装为一个task，提交给MPSCQ，排队等待让NioEventLoop线程执行，而真正的执行时机则放在了NioEventLoop线程事件轮询方法——run里，确切的说是前面分析的runAllTasks()里的safeExecute()方法里，本质就是让NIO线程直接执行Runnable接口的run方法，自然也就执行了register0(promise)

Netty为何要把定时任务聚合到普通任务队列里？



其实，主要还是考虑到编码的复杂度，以及性能问题。因为Netty的外部线程也能通过schedule等方法发起定时任务，这些方法的执行不应该阻塞NioEventLoop线程，故需要先扔到定时任务队列ScheduledTaskQueue排队等待deadline到期后执行，且为了共用Netty已有的线程模型以及线程安全保障机制，在NioEventLoop线程中把定时任务按照规则从scheduledTaskQueue聚合到MPSCQ就是一个自然的想法。因为MPSCQ在NioEventLoop的执行是线程安全的，Netty将外部线程的所有异步task聚集在这里，在NioEventLoop线程内串行执行，不论是定时任务还是普通异步任务，都省去了同步成本，也实现了代码的复用——都不会阻塞NIO线程



```java
ScheduledFutureTask(
    AbstractScheduledEventExecutor executor,
    Runnable runnable, V result, long nanoTime) {

    this(executor, toCallable(runnable, result), nanoTime);
}

class PromiseTask<V> extends DefaultPromise<V> implements RunnableFuture<V> {

    static <T> Callable<T> toCallable(Runnable runnable, T result) {
        return new RunnableAdapter<T>(runnable, result);
    }

    private static final class RunnableAdapter<T> implements Callable<T> {
        final Runnable task;
        final T result;

        RunnableAdapter(Runnable task, T result) {
            this.task = task;
            this.result = result;
        }

        @Override
        public T call() {
            task.run();
            return result;
        }
        ...
    }
    ...
}


// ScheduledFutureTask#run    
@Override
public void run() {
    assert executor().inEventLoop();
    try {
        if (periodNanos == 0) {
            if (setUncancellableInternal()) {
                V result = task.call();
                setSuccessInternal(result);
            }
        } else {
            // check if is done as it may was cancelled
            if (!isCancelled()) {
                task.call();
                if (!executor().isShutdown()) {
                    if (periodNanos > 0) {
                        deadlineNanos += periodNanos;
                    } else {
                        deadlineNanos = nanoTime() - periodNanos;
                    }
                    if (!isCancelled()) {
                        // scheduledTaskQueue can never be null as we lazy init it before submit the task!
                        Queue<ScheduledFutureTask<?>> scheduledTaskQueue =
                            ((AbstractScheduledEventExecutor) executor()).scheduledTaskQueue;
                        assert scheduledTaskQueue != null;
                        scheduledTaskQueue.add(this);
                    }
                }
            }
        }
    } catch (Throwable cause) {
        setFailureInternal(cause);
    }
}
```







以任务本身的执行时机分类，MPSCQ里的task就两类：

1、一类是非定时任务；

2、一类是定时任务



针对这两大类任务，Netty又分为三步处理：

- 先对task的类型分类保存
- 对task进行聚合(本质是都放进MPSCQ)
- 使用NIO线程伺机执行

```java
io.netty.util.concurrent.SingleThreadEventExecutor#execute
```



规避频繁调用时间API的考虑，以及使用位运算代替取模运算

```
runTasks % (0x3F + 1)=runTasks % 0x40 = runTasks % 64 =             runTasks & 0x3F = runTasks & 63
```

每累计执行N个任务后，才真的去计算一次时间

如何优化普通的取模运算

Netty的异步任务的执行结束时机——两个时机，一个是MPSCQ空，一个是超时

调用操作系统的时间API是一个代价较大的行为，应该避免频繁调用。





在并发程序中，当一个方法内需要改变外部的实例变量时，往往将外部的实例变量临时赋值给一个局部变量（即方法内的局部变量），然后对该临时的局部变量进行操作，这样做是为什么呢，难道代码不冗余吗？

般来说，这样做的目的在于：多线程环境下，可以避免某个方法改变实例变量导致意外的BUG。当然这里的改变也可能是调用这个实例变量对应的方法等



确保在方法内操作的始终是这个对象的地址，不论外部线程对packetTemp如何修改，都不会影响到方法内的局部变量

在并发环境下被操作的实例变量可能被改变，需要将该变量赋值给局部变量，对局部变量进行操作，理论基础——Java传参是值传递，在赋值上也是一样的。



#### 适配器模式

**适配器使用场景**



可以将该模式看做是一个补偿机制，应用这种模式算是"无奈之举”，如果设计之初就能规避一些不兼容的问题，那么就不需要使用该模式，但是往往事与愿违，我们无法未卜先知，并且有时候还涉及到权限的问题，一些代码我们无法改变。主要场景如下：



1、增强老代码能力

如果一个类已经稳定存在，你不想（或者说没有权限，比如依赖库）修改它，但是还要为其增加新需求，且这个需求和类的现有接口不能匹配，那么就可以使用适配器模式实现接口的转换以便满足新需求



2、软件的升级

比如升级有缺陷的接口，且该接口已经提供给了用户，我们需要做到无感知升级





3、统一多个类的接口

比如某个功能的实现依赖多个外部系统(或者说类)。通过适配器模式将它们的接口适配为统一的接口，然后就可以使用多态的特性来复用代码逻辑。



4、替换依赖的外部系统

比如当把项目中依赖的一个外部系统替换为另一个外部系统的时候，利用适配器模式，可以减少对代码的改动



5、软件的废弃接口设计

一般我们会标记废弃接口为@Deprecated，此时可以将其内部实现逻辑委托为新的接口实现，让使用它的项目有个过渡期。比如，JDK1.0中遍历集合容器的类Enumeration。JDK2.0 对这个类进行了重构，将它改名为Iterator并对它做了优化。但考虑到如果将Enumeration直接从JDK2.0删除，那使用JDK1.0的项目如果切换到JDK2.0，就会编译不过。为了避免这种情况发生，JDK选择使用适配器模式对废弃接口进行适配。



6、适配不同格式的数据

适配器模式除了用于接口适配，还能用在不同格式的数据之间的适配。比如Java中的Arrays.asList()可以看作一种数据适配器，它将数组类型的数据转化为集合容器类型。







#### 如何优雅地启动一个Netty服务器？

守护线程

1、同步等待—停机

```java
f.channel().closeFuture().sync();
```

2、回调通知—停机（推荐）

```java
ChannelFuture的addListener
GenericFutureListener#operationComplete
```



tailTasks 负责收尾任务的MPSCQ

SingleThreadEventLoop#executeAfterEventLoopIteration(task)

统计执行一次事件循环花了多长时间就可以调用此方法将自己实现的统计业务封装为task在提交给NioEventLoop线程，当NIO线程执行完一次事件循环，就会自动执行这个task。





@UnstableApi是Netty自己定义的注解，意思是表明某个API不是稳定的，可能在后续版本里删除，如果线上业务有依赖这样的方法，那么在升级Netty版本时，需要注意这一点。



confirmShutdown()

SingleThreadEventExecutor#cleanup()



1、读写分离避免加锁的模式应用

2、CAS+自旋的正确用法，比如需要用局部变量缓存全局属性，以及使用原子更新器代替JDK封装的原子类，性能更佳

3、属性的状态标记类型为整型的好处——方便迅速判断当前状态的范围







设计思想上可以学到：

1、任何服务，在使用完毕或者意外退出时，请尽快释放相关的线程池以及上面绑定的资源，比如Netty会释放自己的线程池，以及每个线程绑定的MPSCQ，I/O多路复用器，Channel，发送缓冲区等。推荐设计独立的停机API，并提供异步监听的使用方法。



2、只要涉及到缓存的消息处理或者异步任务排队处理的框架，都应该对积压的消息（任务）设计优雅的停机策略，比如Netty就是把某个NIO线程的MPSCQ处理完后才关闭该队列，而定时任务不处理，个人理解是考虑到定时任务时间的不确定性，故不对它做出承诺。



3、优秀的服务框架在停机时需要对正在发生的读写逻辑设计优雅停机策略，比如可以将它们完整的执行完毕，也可以将来不及执行的任务清理掉（比如定时任务），并给出提示，可以打印日志，Netty就是这么做的。



4、应该允许用户注册一些停机的钩子，类比JDK的JVM关机钩子，Netty没有使用JDK自带的钩子方法，而是自己搞了一套，原因是JDK的钩子方法有一些缺陷，后续分析。



5、在实际项目中，Netty 作为高性能的网络层框架，往往作为基础通信组件，负责各种协议的接入、解析和请求的分发调度等，例如在RPC和分布式服务框架中，往往会使用Netty实现内部私有协议的传输层和协议层。当应用进程优雅退出时，作为通信框架的Netty也需要优雅退出，此时需要结合实际项目的其它组件以及业务场景进行定制，这里限于篇幅就不展开了。



6、Netty优雅停机的退出时间默认是15s，周期为2，用户可以根据需求自己配置。





7、Netty保证了自己的钩子方法一定会被执行到。



下面总结Netty优雅停机细节，它分了两部分：

1、使用线程池（单独调用某个线程的也行）的shutdownGracefully方法，把NIO线程的状态位设置成ST_SHUTTING_DOWN，即标识该NIO线程不再处理新的任务，该方法是异步的，真正的停机逻辑在NioEventLoop里



2、NioEventLoop的run方法在每轮循环结束后，会有一个判断是否收尾的逻辑，根据当前NIO线程状态判断。如果状态是正在关闭中，那么会做三件事：

- 把发送缓冲区中尚未发送或者正在发送的消息发送完(PS.不保证一定能发送完)，并且以后不允许在发送新的消息
- 取消定时任务并清空定时任务队列，使Netty不能再接受新的定时任务
- 把普通的异步任务执行完，把注册到NIO线程的钩子方法都执行完

以上三步执行期间，会穿插NIO其它资源的释放操作，比如当前绑定在NIO线程上的Channel的关闭，绑定在NIO线程的I/O多路复用器的注册的key的解除和自身的关闭。



以上都搞定，EventLoop线程才真正退出，并标记NIO线程状态位为ST_TERMINATED。



###### ChannelOutboundBuffer

Netty自己封装的缓存消息发送队列的数据结构，即一些还没有来得及发出去的消息，都会保存在该对象。该对象正常情况下不为null，因为初始化的时候就new出来了





最好不要完全依赖Netty的优雅停机API，它并不是真正的绝对优雅，后续我会拆解它的整套运行机制的源码，结论是不论你怎么停机，都避免不了丢失定时任务，以及丢失Netty发送缓冲区里缓存的旧消息。比如，即使触发了Netty的优雅退出方法，在Netty优雅退出方法执行期间，应用线程仍然有可能继续调用Channel的相关API发送消息（用户可以随时调用Channel的write系列API发消息），这些新的消息将发送失败，如果是敏感数据，那么就丢失了。而且Netty也一直在升级，其优雅退出的策略一直在调整，故不要单一的依赖它













JVM的钩子机制需要注意的坑：

1、Netty的钩子任务可以被以FIFO的顺序驱动，而JVM的钩子任务的执行顺序无法保证，并且JVM会尽量让它们都并发（甚至并行）运行



2、当所有的钩子任务都结束后，JVM才会正常停止，所以不要在钩子任务里写耗时的、或者有外部资源依赖的业务逻辑，应该让它快速完成。因为当JVM关机时，操作系统层面会有对应的退出流程，JVM关机耗时太久，可能导致钩子无法被完整运行。



3、前面提到在关机开始驱动钩子任务时，程序里的守护线程仍然可以运行



4、所有的钩子任务一旦开始执行，就无法被正常停止，除非用户显式的调用halt这个强制关闭JVM的方法，同理，一旦有钩子开始被执行，就不能在注册新的钩子任务给JVM，也不能取消它们。



5、注册的钩子任务如果有状态，那么需要用户自己保证线程安全。



6、钩子任务不要尝试去使用其它线程，可能会导致死锁，需要严格测试。比如在ShutdownHook中调用System.exit, 会卡住JVM，导致JVM进程无法退出，如下demo在JDK8下确实无法退出JVM。


7、如果要处理钩子里的未捕获异常，那么可以使用线程组的ThreadGroup#uncaughtException方法，该方法会把异常堆栈跟踪打印到System.err和终止它的线程里，并且不会影响JVM的退出



8、在某些情况下并不会被执行，比如JVM自己崩溃、导致它无法接收关机信号，或者Linux用户在外面执行操作系统层面的SIGKILL信号：











实现类似Netty这样的NIO框架时，其线程模型使用单线程的好处是：

1、单线程可以规避NIO的部分API底层竞争锁的操作

2、单线程可以规避线程不安全的NIO API，以减少在外部额外加锁的场景，降低编码实现的复杂度

3、涉及到系统调用，尤其是Java这种跨平台语言，那么在不同操作系统下的实现机制很可能完全不一样。所以设计的比较好的框架都会考虑底层平台的差异带来的问题，Netty就使用了单线程来屏蔽底层操作系统对同步机制的差异，避免了在多个平台的多套实现。



#### TCP 参数

##### tcp 心跳 so_keepalive 不满足需求？

以Linux系统为例，一个Socket文件描述符大概占3Bytes内存，如果服务端是Java程序，那么JVM的Socket对象本身也会占内存，所以3bytes是非常保守的估计了。于是，协议设计者就为TCP协议搞了一个SO_KEEPALIVE参数，并且配套实现了传输层的心跳程序，它会检测TCP链路的空闲时间，如果持续空闲了120分钟，那么服务端的TCP协议栈会主动发一个keepalive报文段给客户端，对方没有回复，默认会重传9次，每次重传间隔是75s，9*75s后客户端仍然没响应，服务端会主动关闭该链路。期间，如果距离上一次传输数据后120分钟内又发生了数据传输，那么TCP心跳程序会将自己的定时器重置，反复执行以上流程。



因为网络传输层只负责通信，它并不关心业务。也就是说TCP的探活程序只能保证链路正常存活，但无法得知通信双方的应用层是否正常，比如通信双方虽然能互相交流数据，但服务端可能已经报了空指针异常，或者OOM了，或者死锁了，响应码5xx了



以Linux为例，它有三个参数可以影响TCP协议的探活程序：

- tcp_keepalive_time 默认7200s：即空闲检测时间，距离上次传送报文段后，7200s未收到新报文段，就开始检测
- tcp_keepalive_intvl 默认75s：即空闲检测后，每隔75s发送一个心跳包
- tcp_keepalive_probes 默认9次：即触发空闲检测后，一共发送9次心跳包，对方仍然未响应，服务端才主动关闭连接

TCP协议的心跳程序是一个可选项，默认不会开启。



```java
.childOption(NioChannelOption.of(StandardSocketOptions.SO_KEEPALIVE),true)
```



StateCheckHandler





ss命令查看此时的套接字信息。ss是Socket Statistics缩写。ss命令可以用来获取socket统计信息，可以显示和netstat类似的内容。

##### SO_BACKLOG

https://mp.weixin.qq.com/s/uCu1QCdzFw692tp5bGD81A

TCP的SO_BACKLOG参数。该参数和TCP的全连接和半连接队列有关。



JDK的bind方法可以附加一个backlog参数，最终会调用操作系统的listen，然后在调用TCP/IP协议栈配置backlog。而当服务端进程调用listen时，服务端的TCP协议栈状态会从CLOSED变为LISTEN态，同时操作系统内核会为该进程创建两个队列数据结构：

1、半连接队列（Incomplete connection queue），又称SYN队列、等待队列

2、全连接队列（Completed connection queue），又称Accept队列、接入（监听）队列



backlog参数描述的是服务器端ESTABLISHED态对应的全连接队列最大长度。注意TCP协议栈不一定采取backlog的值，因为全连接队列大小还被操作系统内核参数somaxconn约束。



全连接队列的长度与如下两个参数有关：

1、net.core.somaxconn是Linux系统的内核参数，在Linux系统中，somaxconn默认128大小，它定义了服务端机器系统级的，每一个端口的全连接队列最大长度

2、应用设置的backlog参数，它定义了服务端的应用层面的全连接队列最大长度





SYN报文段不携带数据，长度为0，但是它消耗一个序列号。



1、针对三次握手：

SYN报文段不携带数据，长度为0，但是它消耗一个序列号。确认序号的意义是包含发送确认的一端所期望收到的下一个序号，另外单纯的ack报文不带数据，长度为0，也不消耗序列号，故不需要被确认，一句话：不消耗序号的报文段不需要确认，否则就是死循环了。



2、三次握手状态迁移

服务端状态 > CLOSED > LISTEN > SYN-RCVD > ESTABLISHED

客户端状态 > CLOSED > SYN-SENT                  > ESTABLISHED



3、backlog参数在哪里配置，作用是？

服务端Socket配置，在应用层面设置服务端口对应的全连接队列大小，该队列大小还会被系统参数somaxconn约束，后者优先级高



4、全连接队列和半连接队列在何时创建，怎么工作的？

当服务端进程调用listen时，服务端的TCP协议栈状态会从CLOSED变为LISTEN态，同时操作系统内核会为该进程的端口创建半连接队列和全连接队列。在服务器回送【SYN+ACK】后，等待客户端的【ACK】期间会存放那时的未完成连接到半连接队列，此时服务器处于SYN_RECV态。全连接队列在服务器收到【ACK】后存放那时的连接，此时服务器处于ESTABLISHED态，要知道每个TCP程序在被监听的端口处都有一个全连接队列（accept queue，又叫监听队列），后续应用中每调用一次accept函数，就会先进先出的（FIFO）从全连接队列里取走一个连接



##### SO_LINGER

关闭TCP连接的方式有两种：

1、发送FIN包给对方，标识自己这端所有数据都已发出，后面不会再发数据，该方式是优雅关闭

2、发送RST包给对方，即强制废掉这个连接，该方式无法做出任何安全性和稳定性的保证。

https://mp.weixin.qq.com/s/3ttVV2JgM_-pSPRmwqiwSQ

2MSL(MSL一般为2分钟)

假设是服务器主动关闭连接，那么服务器的Socket将处于TIME-WAIT态并持续2MSL的时间（通常一个MSL=2分钟）



如果不想等2MSL那么久才关闭连接，应该怎么办？以Netty开发服务器为例，此时可以配置它的SO_LINGER参数，配置为1，表示启用协议栈的Linger特性，并且超时时间为1s：

socket.setSoLinger(boolean,int)



> linger 参数配置：
>
> true 打开：
>
> 1. delay=0；close不阻塞并立即丢弃缓存数据同时发一个RST报文段暴力关闭
> 2. delay>0;close 阻塞，在delay 时间内发送缓冲区数据，超时后会丢弃剩余数据并RST连接



##### SO_REUSEADDR

```jav
.option(ChannelOption.SO_REUSEADDR, true)
```

默认情况这个值都为false，在TCP协议中即表现为0，表示关闭。而透传到Java中，其配置的API在不同JVM有不同实现，为了保险起见，编写TCP或者HTTP的服务端程序，一定要主动设置这个参数为true。



1、SO_REUSEADDR参数在服务端和客户端都能使用并生效

2、必须将绑定同一个端口的所有的Socket对象的SO_REUSEADDR都打开才能起作用

3、在服务端配置，必须在bind方法之前设置才能生效

4、一般都是客户端主动关闭连接，此时客户端用不到SO_REUSEADDR参数，因为客户端一般使用临时端口，端口重用的概率极低

5、使用netstat -laptn命令结合gerp可以查询端口占用情况

6、TCP协议规定TIME-WAIT会一直持续2MSL(即两倍的TCP报文段最大生存时间)，以此来确保旧连接不会对新连接产生影响。而处于TIME-WAIT态的连接资源不会被内核释放，所以作为服务器，尽量不要频繁的主动断开连接，比如可以使用连接池复用技术以减少TIME-WAIT态导致的资源浪费。



##### TCP的Nagle算法以及TCP_NODELAY参数

TCP协议通信，按照数据流的大小，一般分为两种：

- 成块儿的大消息传输
- 交互的短消息传输

针对后者，TCP协议实现了一个算法——Nagle，它通过将发送缓冲区内的小的报文段自动相连，组成较大的报文段一起发送，来阻止大量小报文段的发送增大网络拥塞，从而提高网络应用的发送效率。并且默认是打开的，即参数TCP_NODELAY默认为false。但是对于实时性要求较高的应用(比如telnet、网游等)，大概率需要关闭此算法。







### Netty的空闲检测IdleStateHandler

```java
public class IdleStateHandler extends ChannelDuplexHandler {
    
    private static final long MIN_TIMEOUT_NANOS = TimeUnit.MILLISECONDS.toNanos(1);
    
    // handlerAdded
    @Override
    public void channelActive(ChannelHandlerContext ctx) throws Exception {
        // This method will be invoked only if this handler was added
        // before channelActive() event is fired.  If a user adds this handler
        // after the channelActive() event, initialize() will be called by beforeAdd().
        initialize(ctx);
        super.channelActive(ctx);
    }
    
    private void initialize(ChannelHandlerContext ctx) {
        // Avoid the case where destroy() is called before scheduling timeouts.
        // See: https://github.com/netty/netty/issues/143
        switch (state) {
        case 1:
        case 2:
            return;
        }

        state = 1;
        initOutputChanged(ctx);

        lastReadTime = lastWriteTime = ticksInNanos();
        if (readerIdleTimeNanos > 0) {
            //
            readerIdleTimeout = schedule(ctx, new ReaderIdleTimeoutTask(ctx),
                    readerIdleTimeNanos, TimeUnit.NANOSECONDS);
        }
        if (writerIdleTimeNanos > 0) {
            //
            writerIdleTimeout = schedule(ctx, new WriterIdleTimeoutTask(ctx),
                    writerIdleTimeNanos, TimeUnit.NANOSECONDS);
        }
        if (allIdleTimeNanos > 0) {
            //
            allIdleTimeout = schedule(ctx, new AllIdleTimeoutTask(ctx),
                    allIdleTimeNanos, TimeUnit.NANOSECONDS);
        }
    }
}


private final class ReaderIdleTimeoutTask extends AbstractIdleTask {

        ReaderIdleTimeoutTask(ChannelHandlerContext ctx) {
            super(ctx);
        }

        @Override
        protected void run(ChannelHandlerContext ctx) {
            long nextDelay = readerIdleTimeNanos;
            if (!reading) {
                nextDelay -= ticksInNanos() - lastReadTime;
            }

            if (nextDelay <= 0) {
                // Reader is idle - set a new timeout and notify the callback.
                readerIdleTimeout = schedule(ctx, this, readerIdleTimeNanos, TimeUnit.NANOSECONDS);

                boolean first = firstReaderIdleEvent;
                firstReaderIdleEvent = false;

                try {
                    IdleStateEvent event = newIdleStateEvent(IdleState.READER_IDLE, first);
                    channelIdle(ctx, event);
                } catch (Throwable t) {
                    ctx.fireExceptionCaught(t);
                }
            } else {
                // Read occurred before the timeout - set a new timeout with shorter delay.
                readerIdleTimeout = schedule(ctx, this, nextDelay, TimeUnit.NANOSECONDS);
            }
        }
    }
```



1、ReadTimeoutHandler

如果在指定的时间间隔内，当前Channel没有可读的数据，那么传播ReadTimeoutException，并关闭对应的Channel。可以在自己的ChannelHandler中继承并覆写它的exceptionCaught()方法，来检测ReadTimeoutException，并做一些善后处理

2、WriteTimeoutHandler

如果在指定的时间间隔内，当前Channel没有数据写出（意图），那么传播WriteTimeoutException，并关闭对应的Channel。可以继承覆写它的exceptionCaught()方法来检测WriteTimeoutException，并做善后处理。





1、对于读空闲检测ReadTimeoutHandler，它只是检测有没有数据入站，触发后会传播ReadTimeoutException并关闭Channel，自定义的handler感知到这个异常可以做一些处理操作。



2、对于写空闲检测WriteTimeoutHandler，它只是检测的写操作本身有没有按时完成，如果你write了一个2G的大文件(一时半会写不完）或者说当前网络不佳，发送缓冲区满了，你想感知到真正的一段时间内是否有数据在写，那么可以使用IdleStateHandler并配置observeOutput为true，这样只有在真的没有数据写时，才会触发写空闲检测。因为Netty此时不会认为数据都写入发送缓冲区才算有写操作，只要有写意图就算在写了。如果你想知道一段时间里，这个写操作到底有没有完成，那么你可以使用WriteTimeoutHandler。







###### 空闲检测处理器对写检测的优化



```java
public IdleStateHandler(
            long readerIdleTime, long writerIdleTime, long allIdleTime,
            TimeUnit unit) {
        this(observeOutput:false, readerIdleTime, writerIdleTime, allIdleTime, unit);
    }
```

observeOutput默认是false，它代表数据成功写入Channel的缓冲区，才算有写操作发生，否则只要有写的意图，Netty就认为写操作发生。

```java
private void initialize(ChannelHandlerContext ctx) {
    // Avoid the case where destroy() is called before scheduling timeouts.
    // See: https://github.com/netty/netty/issues/143
    switch (state) {
        case 1:
        case 2:
            return;
    }

    state = 1;
    // 
    initOutputChanged(ctx);

    lastReadTime = lastWriteTime = ticksInNanos();
    if (readerIdleTimeNanos > 0) {
        readerIdleTimeout = schedule(ctx, new ReaderIdleTimeoutTask(ctx),
                                     readerIdleTimeNanos, TimeUnit.NANOSECONDS);
    }
    if (writerIdleTimeNanos > 0) {
        writerIdleTimeout = schedule(ctx, new WriterIdleTimeoutTask(ctx),
                                     writerIdleTimeNanos, TimeUnit.NANOSECONDS);
    }
    if (allIdleTimeNanos > 0) {
        allIdleTimeout = schedule(ctx, new AllIdleTimeoutTask(ctx),
                                  allIdleTimeNanos, TimeUnit.NANOSECONDS);
    }
}

/**
如果用户配置observeOutput为true（默认为false），那么就提前获取当前Channel的发送缓冲区对象ChannelOutboundBuffer的一个快照，ChannelOutboundBuffer是Netty在应用层实现的一个发送缓冲区，是一个容纳Netty调用了write后的数据的容器，存储的对象是调用write或者writeAndFlush方法打算发出去的消息，它底层基于链表实现，链表的节点是Netty自定义的Entry对象，每个Channel都绑定了一个ChannelOutboundBuffer对象，该缓冲区全程只会被NIO线程执行
*/
private void initOutputChanged(ChannelHandlerContext ctx) {
    if (observeOutput) {
        Channel channel = ctx.channel();
        Unsafe unsafe = channel.unsafe();
        ChannelOutboundBuffer buf = unsafe.outboundBuffer();

        if (buf != null) {
            lastMessageHashCode = System.identityHashCode(buf.current());
            // TOTAL_PENDING_SIZE_UPDATER
            lastPendingWriteBytes = buf.totalPendingWriteBytes();
            lastFlushProgress = buf.currentProgress();
        }
    }
}
```



将需要频繁创建的同一个逻辑的回调监听器搞为常量，后续复用这一个即可，降低GC的压力。我们在实际开发中，也应该尽量复用可以重复使用的组件或者对象。



##### HashedWheelTimer对时间轮算法的实现

> ```
> Do not create many instances.HashedWheelTimer creates a new thread whenever it is instantiated and started.Therefore,you should make sure to create only one instance and share it across your application.One of the common mistakes,that makes your application unresponsive,is to create a new instance for every connection.
> ```

底层数据结构是一个叫wheel的数组，它本质就是一个哈希表，时间轮的默认大小是512，即wheel数组默认512大小，数组的元素位置就是时间轮里所谓的槽位，你可以配置槽位的时间间隔，Netty默认的配置是一个槽位代表100ms



TimerTask

HashedWheelTimer有三个核心的内部类：

Worker，负责扫描时间轮的槽位，负责添加新的定时任务，执行定时任务等



HashedWheelBucket



HashedWheelTimeout 内部本身聚合了next和prev指针，主要是负责保存用户提交的定时任务以及一些配置，这里就和Hashmap的数组+链表结构非常像了。 



参数leakDetection，默认true，代表Netty会为时间轮配置并开启内存泄漏检查工具，如果配置为false，并且worker线程没有被用户单独配置为后台线程，那么内存泄漏检查工具仍然会被打开。这里的设计目的就是监控时间轮内存泄露情况



tick是在HashedWheelTimer内部设置的一个变量，负责记录workerThread一共扫描了多少个槽位，同时workerThread也负责执行到期的定时任务，和添加合适的定时任务到指定的wheel槽位中。



Netty的时间轮机制的设计思路，从时间轮的创建，添加定时任务，调度执行

##### 理解NIO的Buffer组件，为何Netty没有使用？

NIO中所有的数据都是用Buffer装载，它提供了对数据的结构化访问以及维护读写位置等功能。具体的说，Buffer就是一块内存，它底层是数组，我们知道数组既能读也能写，且可以反复使用，这是比JDK的I/O流先进的地方之一，JDK的I/O流要么只能读，要么只能写，二者不可兼备，并且无法重复读，NIO的Buffer就没有这些缺陷。



因为JDK的Buffer，尤其是ByteBuffer的使用很复杂，性能也不佳。比如每次写完都需要转换读写模式，而且不能自动扩容，也没有池化的设计，读写索引是一个指针，维护起来也不方便，基于此现实原因，Netty设计封装了一个ByteBuf类来替代NIO的ByteBuffer，它既解决了JDK的API的局限性，又为网络程序的开发提供了更便捷的API。

#####  directbytebuffer



Cleaner继承了JDK的虚引用类——PhantomReference

自动清理堆外内存的task——Deallocator类

ReferenceQueue ReferenceHandler



、DirectByteBuffer——堆外内存优势和缺陷：

- 不受限GC的管理（严格说Java设计了通过GC去清理堆外内存的机制，只不过不耽误这种优势），无全局停顿的问题，因为这块儿内存在堆外，GC的各种算法管不到它的头上，GC唯一能做的就是free它而已。
- 减少了用户空间数据的拷贝次数——避免了数据在Java堆和Native堆中来回复制数据。注意它和操作系统的“零拷贝”是两码事
- 依然会OOM，需要格外小心的使用，虽然它不会受到Java堆大小的限制，但是既然是内存，肯定还是会受到本机总内存大小以及处理器寻址空间的限制，而且一般服务器配置虚拟机参数时，会根据实际内存去设置-Xmx等参数信息，但经常忽略掉堆外内存的配置，从而在依赖了使用了堆外内存的框架时，高并发场景下容易出现堆外的OutOfMemoryError异常
- 堆外内存分配过程相比堆内存分配，比较损耗性能，所以Netty为其实现了内存池，以减少分配和回收堆外内存的次数，当然这个池子是一个通用池，堆内内存也能使用。



2、堆外内存的分配本质就是C语言的malloc函数，分配的是一段连续的虚拟内存，是在用户态分配，这块儿内容需要操作系统基础来理解，不多说。



3、需要知道堆外内存的回收机制——虚引用+单独的清理线程，当然用户也可以手动释放，本质都是调用了C语言的free函数。

##### “零拷贝”的硬件基石——DMA到底是个什么东西，它是如何工作的？

https://mp.weixin.qq.com/s/boRw6dTY8arnxCe_NVFEig

广义的操作系统的零拷贝不仅是用户空间和内核空间的数据拷贝的消除，在内核空间范围内的不必要的拷贝次数也要尽量减少，还要尽量解放CPU，比如使用DMA这种硬件设备来代替CPU完成一些数据拷贝动作，所以操作系统零拷贝离不开DMA技术的支持，否则“零”拷贝就无从谈起。



DMA是一种硬件的技术，它的全称叫直接内存访问(Direct Memory Access)技术。

DMA最有价值的地方体现在当要传输的数据特别大、还要求速度特别快的时候，可以有效减少CPU的阻塞时间。

DMA的实际样子就是主板上的一个芯片，本质是DMA控制器(DMAController,简称DMAC)，并且在各个I/O设备上都加了DMAC芯片



零拷贝的实现在Linux里就是mmap系统调用。还有一种零拷贝的实现在Linux里就是sendfile系统调用，它的目的是在两个文件描述符之间直接传递数据(完全在内核中操作)，不需要将数据拷贝或者映射到虚拟地址和物理地址同时可见的内存空间中去，所以它只适用于应用不需要对所访问数据进行额外处理的场景。

如果该Linux不支持该机制，那么JDK会调用到transferToTrustedChannel方法

MappedByteBuffer，是JDK NIO里提供的一个文件内存映射工具类，它可以实现虚拟地址和物理页地址空间的映射，即能避免用户空间到内核空间的数据拷贝





Java中提供了API——FileChannel的transferTo()方法实现上述操作系统的零拷贝功能，

JDK注释到：字节数据可以通过调用该API实现从一个文件传输到另一个文件，和它匹配的还有一个transferFrom方法

如果还不支持，那么就使用兜底的方案传输——transferToArbitraryChanne



Linux的网络收发涉及到的多个缓冲区进行拆解，这些缓冲区包括了：

1、网卡收、发数据帧时，会基于DMA设备拷贝数据，此时涉及到了和DMA进行交互的缓冲区，该缓冲区是一个环形缓冲区

2、网卡发出硬中断，网卡驱动会在内核分配sk_buff缓冲区

3、应用空间的程序可以调用语言的Socket API，调用系统调用将数据从用户空间和内核的Socket缓冲区（发送或者接受）之间来回搬运



以上，和DMA交互的环形缓冲区存在于网卡驱动的管控范围，这里无论如何都要涉及到真正的数据移动——即存在拷贝过程。



而网卡驱动在内核里创建的sk_buff缓冲区，它本质是一个双向链表结构，链表里的元素就是一个个的网络帧，所以在内核的TCP/IP协议栈处理这些帧时，这些网络帧实际上不需要移动，只需要修改链表的指针即可实现在不同协议层传递，这里不存在拷贝过程。



再看内核套接字Socket的缓冲区，所谓的Socket发送和读取数据，本质是将数据写入Socket缓冲区和从缓冲区里读数据。此时阻塞的API就会解除阻塞，从内核拷贝数据到应用空间，或者反之。



##### 零拷贝的另一种优化：JDK（Netty）封装的I/O汇聚和发散

> 许多操作系统能把组装／分解过程进行得更加高效。根据发散／汇聚（Scatter/Gather）的概念，进程只需一个系统调用就能把一连串缓冲区地址传递给操作系统。然后内核就可以顺序填充或排干多个缓冲区，读的时候就把数据发散到多个用户空间缓冲区，写的时候再从多个缓冲区把数据汇聚起来这样用户进程就不必多次执行系统调用（那样做可能代价不菲），内核也可以优化数据的处理过程，因为它已掌握待传输数据的全部信息。如果系统配有多个CPU，甚至可以同时填充或排干多个缓冲区。
>
> 网络素材

Netty刷新消息有三种策略，其实第2、3种策略差不多。

1、当刷新的不是普通的ByteBuf，此时直接调用上面黄色1处的doWrite0方法刷新数据，比如写文件

2、当nioBuffers[]的长度等于1，说明是单次刷新一个缓冲区，通过nioBuffers[0]获取待发送的ByteBuffer对象，在黄色2处调用JDK的SocketChannel的write方法完成刷新操作，在非阻塞模式下该write方法是非阻塞的，所以不论数据能否成功写入TCP的缓冲区，write都会立即返回，如果返回的是0，那么说明此时可能网络不佳，TCP的发送缓冲区已满，无法写入新数据，此时Netty会在上面的黄色3处做出判断和处理——及时提前结束自旋发送，并为当前Channel注册OP_WRITE事件。因为此时继续刷新消息已经没有任何意义了，反而导致CPU空转，性能下降，具体参考文章[恶劣的网络环境下，Netty是如何处理写事件的？](http://mp.weixin.qq.com/s?__biz=MzU1NjY0NzI3OQ==&mid=2247484674&idx=1&sn=8a78fc7a73d543fd4af3ceda6e76fb4c&chksm=fbc09302ccb71a143459593164d1e786e42b9a6a1bad8c76a86d483e43c0f64cbced9d6811d6&scene=21#wechat_redirect)如果成功刷新数据到TCP缓冲区，那么在黄色4处重新评估下次可发送的数据量并且清理Netty应用层发送缓冲区里的一个数据链表（Entry）里已刷新出去的消息对象节点（此时会对堆外内存进行释放），自旋次数减1，此处是Netty的自适应发送缓冲区机制，暂且不表，抓主线。

3、当ByteBuffer[]的长度大于1,说明是批量刷新缓冲区，走default分支，即在黄色5处将nioBuffers数组批量写入TCP发送缓冲区，此时就是利用了NIO的汇聚和发散功能，后续的黄色6和7处的套路和前面黄色3和4的作用一样，不再赘述。



JDK对I/O的汇聚发散的实现原理了，本质是封装了Linux的writev函数

《Unix环境高级编程》的解释：readv和writev函数用于在一次函数调用中读、写多个非连续的缓冲区。有时也将这两个函数称为散布读（scatter read）和聚集写（gather write）。

##### C/C++

C/C++规定，void *类型可以强转为任何其他类型的的指针。

malloc





#### NIO提供的文件内存映射工具——MappedByteBuffer

https://mp.weixin.qq.com/s/QIuS2E4EHMEnEC-hN9SJCQ



```java
MappedByteBuffer mappedByteBuffer = 
fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 5);

public abstract MappedByteBuffer map(MapMode mode,
                                         long position, long size)
        throws IOException;
```



本质就是Linux的mmap，然后将分配好的内存映射区域的首地址返回给Java即可



##### Netty 刷新数据

Netty的write写过程:

1、按照出站顺序，传播write事件，执行各个出站处理器的write方法，最终到达pipeline头结点的write方法，调用unsafe的write操作

2、Netty默认对I/O操作分配池化的堆外内存，如果发送的消息分配的不是堆外内存，那么内部就会转化成堆外内存，所以建议在编写代码时，执行I/O操作直接使用堆外内存，否则底层还是会转换，耽误时间。

3、将要发送的消息写入发送缓冲区ChannelOutboundBuffer：本质是ByteBuf被封装成缓冲区的内部对象——Entry，同时使用尾插法插入发送缓冲区内部的Entry链表

4、Netty有流控机制，原理是每次加入新的Entry时，会拿到当前总共待发送的消息字节数，然后比较高水位线，并设置和传播写状态。目的是待发送消息不能一直写入发送缓冲区，防止自身OOM或者下游被打挂，Netty就提供了一个高水位线进行流控（对应的，还有一个低水位线，后续总结），超过高水位（默认=64KB）就设置当前Channel为不可写，同时传播不可写事件，方便用户兜底处理。



Netty刷新数据到网络的流程：

1、遍历发送缓冲区ChannelOutboundBuffer的Entry链表里刚刚被添加的待发送消息节点，设置刷新的标志（发送缓冲区的几个指针的变化，后续专题拆解），过滤已经被取消发送的消息，同时设置当前待刷新的消息为不可取消

2、判断低水位线（默认的LOW_WATER MARK=32K），决定是否改变当前Channel的可写状态并传播该事件

3、将Netty的ByteBuf对象（待刷新的消息对象）转换为JDK NIO的Buffer，才能调用JDK的API刷新数据

4、通过自旋+自适应发送缓冲区+延时发送+OP_WRITE事件驱动+NIO的汇聚，优化刷新数据的流程，而本质是分策略的调用JDK的Socket的write方法刷数据到TCP发送缓冲区，这里的策略包括了普通消息的刷新，也包括了批量刷新（NIO的汇聚），和文件发送。

5、自动清理堆外内存和删除ChannelOutboundBuffer的Entry链表中已刷新的节点，解除用户后顾之忧。

以上，也能知道Netty的write方法可以调用多次——仅仅是添加数据到应用层的发送缓冲区ChannelOutboundBuffer，而flush才是真正的刷新数据到Socket的发送缓冲区，让操作系统执行网络传输动作，所以可以结合业务场景，酌情考虑是调用N次write+1次flush，还是每次都调用writeAndFlush。



##### Netty读消息时，其接收缓冲区是如何优化的，里面体现了哪些设计思想？

https://mp.weixin.qq.com/s/tUlGJS6rFwUCm7MAqQtCnw

AdaptiveRecvByteBufAllocator 自适应的接收缓冲区分配器









### HTTP 范围请求（断点续传的技术原理）

传输上G的超大文件，除了考虑内存外，还要考虑可能的传输失败的场景，或者说想跳过某些分片的场景，比如视频网站里的跳过片头片尾，透传到HTTP协议，就是直接通过技术手段获取了某些范围内的分片数据，HTTP协议为了满足这样的需求就设计了"范围请求”(range requests)技术，它允许客户端在请求头里使用专用字段来表示只获取文件的一部分，相当于是客户端的”化整为零”，而分块传输是服务器的”化整为零”。



注意：范围请求不是服务器必备的功能，可以实现也可以不实现，所以服务器必须在响应头里使用字段"Accept-Ranges:bytes"明确告知客户端”我支持范围请求”。如果不支持的话，那么服务器可以发送"Accept-Ranges:none”或者干脆不发送"Accept-Ranges"字段，这样客户端就认为服务器没有实现范围请求功能，只能老老实实地收发整块文件，以上"Accept-Ranges:bytes"中的bytes=x-y，都以字节为单位，xy表示"偏移量”，从0计数，例如前10个字节表示为"0-9”，第二个10字节表示为"10-19”，而"0-10”实际上是前11个字节。



服务器收到Range字段后，需要做3件事；

- 检查范围是否合法，比如文件只有100个字节，但请求"200-300”。服务器就会返回状态码=416，意思是”范围请求有误”

- 检测范围正确后，服务器根据Range头计算偏移量——读取文件的片段，返回状态码=206，表示返回的body只是原数据的一部分

- 服务器响应消息里要添加一个响应头字段Content-Range，告诉片段的实际偏移量和资源的总大小，格式是"bytes x-y/length”，与Range头区别在没有"=”，范围后多了总长度length。例如对于"0-10”的范围请求，值就是"bytes 0-10/100”


有了范围请求后，HTTP处理视频这类文件时可以根据时间点计算出文件的Range，用户不用下载整个文件，就能精确获取片段所在的数据内容，实现诸如快进，快退，不看片头片尾等功能，而且下载软件常用的断点续传技术，多段多线程并行下载等，也是基于该技术实现



DefaultMessageSizeEstimator

RECYCLER对象，它涉及到了Netty的对象池技术

SO_SNDBUF



自旋写+优先堆外+发送缓冲区设计+自适应缓冲区+延时发送+OP_WRITE事件驱动+NIO的汇聚+流控+内存池+自动释放堆外内存+避免重复flsuh，共同作用，来优化写数据和刷新数据的流程

应用程序有自己的发送缓冲区，TCP也有自己的发送缓冲区，JAVA中可以设置Socket的SO_SNDBUF选项





SO_RCVBUF和SO_SNDBUF















































































































































